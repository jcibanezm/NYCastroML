{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import NullFormatter, NullLocator, MultipleLocator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Chapter 3: Probability and Statistical Distributions\n",
      "\n",
      "## Notation\n",
      "* *random variable*: a variable whose value results from the measurement of a quantity that is subject to random variations. In the text, both random variables and specific realizations (variates) are denoted with lowercase letters: $x$ or $y$ <br><br>\n",
      "* *probability density functions* (PDFs) ascribe a probability to each outcome of a random variable. Denoted with lowercase letters: $p(x)$ <br><br>\n",
      "* *cumulative probability distributions* (CDFs) are denoted with upercase letters: $P(x)$ <br><br>\n",
      "* $p$ is used both for the PDF, $p(x)$, and for the probability of a specific event, $p(x=k)$. <br><br>\n",
      "* *data* are specific, \"measured,\" values of random variables. A single measured value is denoted $x_i$. A set of $N$ measurements is denoted {$x_i$}.\n",
      "\n",
      "## Section 3.1. Probability and Random Variables\n",
      "\n",
      "### 3.1.1. Probability Axioms\n",
      "1.  $p(A)\\ge0$ for each $A$. <br><br>\n",
      "2.  $p(\\Omega)=1$, where $\\Omega$ is the set of all outcomes {$A_i$}. <br><br>\n",
      "3.  If $A_1$, $A_2$, ... are disjoint events, then $p(\\bigcup_{i=1}^{\\infty}A_i) = \\sum_{i=1}^{\\infty}p(A_i)$, where $\\bigcup$ stands for \"union.\"\n",
      "\n",
      "From these, we get the well-known relations:\n",
      "\n",
      "* The sum rule: $p(A\\cup B)=p(A)+p(B)-p(A\\cap B)$ <br><br>\n",
      "\n",
      "![prob_sum](http://www.pha.jhu.edu/~ograur1/astroML/fig_prob_sum.jpeg)\n",
      "\n",
      "\n",
      "* $p(A) + p(\\bar{A}) = 1$ <br><br>\n",
      "* The probability that both $A$ and $B$ will happen: <br><br> \n",
      "   $$p(A \\cap B) = p(A|B)p(B) = p(B|A)p(A)$$ <br>\n",
      "* If the $i=1,\\ldots,N$ $B_i$ events are disjoint and their union is the set of all outcomes, then the law of total probability is<br><br>\n",
      "   $$p(A)=\\sum_ip(A \\cap B_i) = \\sum_i p(A|B_i)p(B_i).$$ <br>\n",
      "   This law works for conditional probabilities as well. Assuming that an event $C$ is not mutually exclusive with $A$ or any of $B_i$, then <br><br>\n",
      "   $$p(A|C) = \\sum_i p(A|C \\cap B_i) p(B_i | C).$$ <br>\n",
      "\n",
      "### 3.1.3. Conditional Probability and Bayes's Rule\n",
      "If two random variables are independent, it follows that <br><br>\n",
      "$$p(x,y) = p(x|y)p(y) = p(y|x)p(x).$$ <br>\n",
      "The *marginal probability* is defined as <br><br>\n",
      "$$p(x) = \\int p(x,y)dy = \\int p(x|y)p(y)dy$$ <br>\n",
      "\n",
      "![conditional_prob](http://www.pha.jhu.edu/~ograur1/astroML/fig_conditional_prob.jpeg)\n",
      "\n",
      "### Bayes's Rule\n",
      "The above equations can be combined to give *Bayes's rule*: <br><br>\n",
      "$$\\large p(y|x) = \\frac{p(x|y)p(y)}{p(x)} = \\frac{p(x|y)p(y)}{\\int(x|y)p(y)dy}$$ <br>\n",
      "Bayesian statistics will be discussed in detail in Chapter 5.\n",
      "\n",
      "### 3.1.4. Transformations of Random Variables\n",
      "Quite often, instead of working with the PDF of the random variable $x$, $p(x)$, we'll find ourselves working with some function of the random variable, $y(x)$. <br>\n",
      "If we know $p(x)$, we can find the PDF $y(x)$. <br>\n",
      "Given that $y=\\Phi(x)$, so that $x=\\Phi^{-1}(y)$, the transformation is <br><br>\n",
      "$$\\large p(y) = p\\left[\\Phi^{-1}(y)\\right]\\left|\\frac{d\\Phi^{-1}(y)}{dy}\\right|$$ <br>\n",
      "For example, if $p(x)=1$ for $0 \\le x \\le 1$, and $y=exp(x)$ so that $1 \\le y \\le e$, then: <br>\n",
      "$$\\large p(y) = \\frac{p[ln(y)]}{y}$$ <br>\n",
      "\n",
      "![transform](http://www.pha.jhu.edu/~ograur1/astroML/fig_transform.png)\n",
      "\n",
      "## Section 3.2. Descriptive Statistics\n",
      "Eight basic statistics used to characterize PDFs: <br><br>\n",
      "\n",
      "- Arithmetic mean (or Expectation value) <br><br>\n",
      "$$\\mu = E(x) = \\int_{-\\infty}^{\\infty}xh(x)dx$$ <br>\n",
      "\n",
      "- Variance <br><br>\n",
      "$$V = \\int_{-\\infty}^{\\infty}(x-\\mu)^2 h(x) dx$$ <br>\n",
      "\n",
      "- Standard deviation <br><br>\n",
      "$$\\sigma = \\sqrt{V}$$ <br>\n",
      "\n",
      "- Skewness <br><br>\n",
      "$$ \\Sigma = \\int_{-\\infty}^{\\infty} \\left(\\frac{x-\\mu}{\\sigma}\\right)^4 h(x) dx$$ <br>\n",
      "Distributions with a long tail toward $x$ larger than the \"central location\" have positive skewness. <br><br>\n",
      "\n",
      "- Kurtosis <br><br>\n",
      "$$ K = \\int_{-\\infty}^{\\infty}\\left(\\frac{x-\\mu}{\\sigma}\\right)^4 h(x)dx -3$$ <br>\n",
      "Highly-peaked PDFs, relative to a Gaussian, have positive kurtosis. <br><br>\n",
      "\n",
      "- Absolute deviation around $d$ <br><br>\n",
      "$$\\delta = \\int_{-\\infty}^{\\infty}|x-d| h(x)dx$$ <br>\n",
      "\n",
      "- Mode - the value of $x$, $x_m$, at which $p(x) = max(p)$ <br><br>\n",
      "$$\\left(\\frac{dh(x)}{dx}\\right)_{x_m} = 0$$ <br>\n",
      "\n",
      "- $p$% quantiles, or percentiles, $q_p$ <br><br>\n",
      "$$ \\frac{p}{100} = \\int_{-\\infty}^{q_p} h(x) dx$$ <br>\n",
      "\n",
      "![kurtosis](http://www.pha.jhu.edu/~ograur1/astroML/fig_kurtosis_skew.jpeg)\n",
      "\n",
      "### 3.2.2. Data-Based Estimates of Descriptive Statistics\n",
      "The above statistics, when they describe the PDF itself, are called *population statistics*. We can _estimate_ these staistics using estimators, or *sample statistics*. <br><br>\n",
      "In general, if each of the above statistics can be written as $\\int_{-\\infty}^{\\infty} g(x) h(x) dx$, then the estimator can be written as $\\sum_i^N g(x_i)$ with a proportionality constant $\\sim(1/N)$. <br><br>\n",
      "\n",
      "- sample arithmetic mean (or esitmator of the mean) <br><br>\n",
      "$$ \\bar{x} = \\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^N x_i $$ <br>\n",
      "\n",
      "- sample standard deviation <br><br>\n",
      "$$ s = \\hat{\\sigma} = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N (x_i - \\bar{x})^2} $$ <br>\n",
      "\n",
      "These estimators have their own associated uncertainties: <br><br>\n",
      "\n",
      "$$ \\sigma_{\\bar{x}} = \\frac{s}{\\sqrt{N}} $$ <br>\n",
      "\n",
      "$$ \\sigma_s = \\frac{s}{\\sqrt{2(N-1)}} = \\frac{1}{\\sqrt{2}} \\sqrt{\\frac{N}{N-1}} \\sigma_{\\bar{x}} $$ <br>\n",
      "\n",
      "$$ \\sigma_{q_p} = \\frac{1}{h_p} \\sqrt{\\frac{p(1-p)}{N}} $$ <br>\n",
      "where $p$ is the $p$th percentile and $h_p$ is the value of the PDF $h(x)$ at that percentile. <br><br>\n",
      "\n",
      "Estimators have *bias*, *variance*, and *efficiency*. <br>\n",
      "- *Bias* is defined as the expectation value of the difference between the estimator and its true value, e.g., $E(\\hat{x} - \\mu)$. <br><br>\n",
      "- Estimators whose bias and variance vanish as $N \\to \\infty$ are called *consistent*. <br><br>\n",
      "- *Efficiency* measures how large a sample is required to obtain a given accuracy. <br><br>\n",
      "\n",
      "#### Interesting note\n",
      "In the case of real data, which contain outliers, quantiles offer a more robust method of determining location and scale parameters. I.e., the *median* and *interquartile ranges* (such as $q_{75} - q_{25}$) are less affected by outliers in the data, as opposed to the *mean* and the *standard deviation*. These are also better for PDFs that do not have finite variance, such as the Cauchy distribution. <br><br>\n",
      "However, these estimators are less efficient (i.e., require more data to be as accurate) and some require more computing time. <br><br>\n",
      "The interquartile range is sometimes renormalized so that the width estimator $\\sigma_G$ becomes an unbiased estimator of the Gaussian $\\sigma$: <br><br>\n",
      "$$ \\sigma_G = 0.7413(q_{75} - q_{25}) $$ <br>\n",
      "$$ \\sigma_{\\sigma_G} = \\hat{\\sigma_G} = 1.06\\frac{s}{\\sqrt{N}} $$ <br>\n",
      "so that $\\sigma_{\\sigma_G} \\sim 1.5 \\sigma_s $\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}